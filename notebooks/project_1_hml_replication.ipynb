{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6133b6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Notebook Setup Complete ---\n",
      "Project Root: /Users/choyounghwan/Documents/GitHub/qr-factor-replication\n",
      "Loading data from: /Users/choyounghwan/Documents/GitHub/qr-factor-replication/data/raw/jkp_hml_raw_data_usa.parquet\n",
      "Data loaded successfully.\n",
      "Data Shape: (4262872, 6)\n",
      "\n",
      "--- Data Head (First 5 Rows) ---\n",
      "shape: (5, 6)\n",
      "┌─────────────────────┬──────────────┬────────┬───────┬────────────┬────────────────┐\n",
      "│ eom                 ┆ id           ┆ permno ┆ be_me ┆ me         ┆ ret_exc_lead1m │\n",
      "│ ---                 ┆ ---          ┆ ---    ┆ ---   ┆ ---        ┆ ---            │\n",
      "│ datetime[ns]        ┆ f64          ┆ f64    ┆ f64   ┆ f64        ┆ f64            │\n",
      "╞═════════════════════╪══════════════╪════════╪═══════╪════════════╪════════════════╡\n",
      "│ 1962-02-28 00:00:00 ┆ 1.01127101e8 ┆ null   ┆ null  ┆ null       ┆ null           │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00112401e8 ┆ null   ┆ null  ┆ null       ┆ null           │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00115301e8 ┆ null   ┆ null  ┆ null       ┆ null           │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00115701e8 ┆ null   ┆ null  ┆ 372.722384 ┆ 0.062274       │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00117701e8 ┆ null   ┆ null  ┆ null       ┆ null           │\n",
      "└─────────────────────┴──────────────┴────────┴───────┴────────────┴────────────────┘\n",
      "\n",
      "--- Summary Statistics ---\n",
      "shape: (9, 4)\n",
      "┌────────────┬───────────────┬──────────────┬────────────────┐\n",
      "│ statistic  ┆ be_me         ┆ me           ┆ ret_exc_lead1m │\n",
      "│ ---        ┆ ---           ┆ ---          ┆ ---            │\n",
      "│ str        ┆ f64           ┆ f64          ┆ f64            │\n",
      "╞════════════╪═══════════════╪══════════════╪════════════════╡\n",
      "│ count      ┆ 3.377571e6    ┆ 4.181393e6   ┆ 4.13993e6      │\n",
      "│ null_count ┆ 885301.0      ┆ 81479.0      ┆ 122942.0       │\n",
      "│ mean       ┆ 1.126572      ┆ 2109.482543  ┆ 0.014971       │\n",
      "│ std        ┆ 171.871991    ┆ 21553.692437 ┆ 8.203588       │\n",
      "│ min        ┆ 6.5937e-7     ┆ 0.0          ┆ -1.014149      │\n",
      "│ 25%        ┆ 0.324037      ┆ 17.81725     ┆ -0.06926       │\n",
      "│ 50%        ┆ 0.617208      ┆ 78.255       ┆ -0.003499      │\n",
      "│ 75%        ┆ 1.054296      ┆ 441.85       ┆ 0.066522       │\n",
      "│ max        ┆ 174620.851064 ┆ 3.7853e6     ┆ 15348.999304   │\n",
      "└────────────┴───────────────┴──────────────┴────────────────┘\n",
      "\n",
      "--- Missing Value Counts ---\n",
      "shape: (1, 3)\n",
      "┌────────┬───────┬────────────────┐\n",
      "│ be_me  ┆ me    ┆ ret_exc_lead1m │\n",
      "│ ---    ┆ ---   ┆ ---            │\n",
      "│ u32    ┆ u32   ┆ u32            │\n",
      "╞════════╪═══════╪════════════════╡\n",
      "│ 885301 ┆ 81479 ┆ 122942         │\n",
      "└────────┴───────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# --- HML Factor Replication and Analysis ---\n",
    "#\n",
    "# Objective:\n",
    "# This notebook replicates the Fama-French Value (HML) factor using the\n",
    "# Jensen, Kelly, and Pedersen (JKP) global factor dataset. The process involves:\n",
    "# 1. Loading the pre-processed data.\n",
    "# 2. Constructing decile portfolios based on the Book-to-Market (be_me) characteristic.\n",
    "# 3. Calculating the monthly return of a long-short HML portfolio.\n",
    "# 4. Validating the replicated factor against the Ken French benchmark.\n",
    "#\n",
    "# Professional Workflow Notes:\n",
    "# - All data ingestion is handled by the `src/data_ingestion.py` script.\n",
    "# - This notebook is for exploratory analysis, portfolio construction, and visualization.\n",
    "# - The environment is managed via Conda (`qr_env`).\n",
    "\n",
    "# --- 1. Imports and Setup ---\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import wrds\n",
    "\n",
    "print(\"--- Notebook Setup Complete ---\")\n",
    "\n",
    "# --- 2. Load the Downloaded Data ---\n",
    "# Define the path to our data relative to the project root.\n",
    "# This makes the notebook portable and runnable on any machine.\n",
    "try:\n",
    "    project_root = Path(__file__).resolve().parent.parent\n",
    "except NameError:\n",
    "    # If running interactively in a notebook, __file__ is not defined.\n",
    "    # We can find the project root by looking for the .git directory.\n",
    "    current_dir = Path.cwd()\n",
    "    while not (current_dir / \".git\").exists():\n",
    "        current_dir = current_dir.parent\n",
    "    project_root = current_dir\n",
    "\n",
    "# Path to the raw data file we downloaded\n",
    "data_path = project_root / \"data\" / \"raw\" / \"jkp_hml_raw_data_usa.parquet\"\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "# Load the data using Polars\n",
    "try:\n",
    "    df = pl.read_parquet(data_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    print(f\"Data Shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load data: {e}\")\n",
    "    df = None # Ensure df is defined even on failure\n",
    "    \n",
    "# --- 3. Initial Data Inspection (Sanity Check) ---\n",
    "if df is not None:\n",
    "    # Display the first few rows to understand the structure\n",
    "    print(\"\\n--- Data Head (First 5 Rows) ---\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Display summary statistics for the key columns\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(df.select([\"be_me\", \"me\", \"ret_exc_lead1m\"]).describe())\n",
    "\n",
    "    # Check for missing values in our key columns\n",
    "    print(\"\\n--- Missing Value Counts ---\")\n",
    "    print(df.select([\"be_me\", \"me\", \"ret_exc_lead1m\"]).null_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92166ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaning Data ---\n",
      "Initial row count: 4,262,872\n",
      "Row count after dropping nulls: 3,341,501\n",
      "Final cleaned row count: 3,341,501\n",
      "Total rows dropped: 921,371 (21.61%)\n",
      "\n",
      "--- Missing Value Counts After Cleaning ---\n",
      "shape: (1, 3)\n",
      "┌───────┬─────┬────────────────┐\n",
      "│ be_me ┆ me  ┆ ret_exc_lead1m │\n",
      "│ ---   ┆ --- ┆ ---            │\n",
      "│ u32   ┆ u32 ┆ u32            │\n",
      "╞═══════╪═════╪════════════════╡\n",
      "│ 0     ┆ 0   ┆ 0              │\n",
      "└───────┴─────┴────────────────┘\n",
      "\n",
      "--- Cleaned Data Head ---\n",
      "shape: (5, 6)\n",
      "┌─────────────────────┬──────────────┬────────┬──────────┬─────────────┬────────────────┐\n",
      "│ eom                 ┆ id           ┆ permno ┆ be_me    ┆ me          ┆ ret_exc_lead1m │\n",
      "│ ---                 ┆ ---          ┆ ---    ┆ ---      ┆ ---         ┆ ---            │\n",
      "│ datetime[ns]        ┆ f64          ┆ f64    ┆ f64      ┆ f64         ┆ f64            │\n",
      "╞═════════════════════╪══════════════╪════════╪══════════╪═════════════╪════════════════╡\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00123901e8 ┆ null   ┆ 0.016308 ┆ 58.499885   ┆ 0.057841       │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00139101e8 ┆ null   ┆ 0.991209 ┆ 53.944242   ┆ -0.001989      │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00146801e8 ┆ null   ┆ 0.312158 ┆ 88.868577   ┆ -0.019436      │\n",
      "│ 1962-01-31 00:00:00 ┆ 1.00165501e8 ┆ null   ┆ 0.444355 ┆ 11.159995   ┆ 0.17543        │\n",
      "│ 1962-03-31 00:00:00 ┆ 1.00756201e8 ┆ null   ┆ 0.527964 ┆ 1053.713768 ┆ -0.007645      │\n",
      "└─────────────────────┴──────────────┴────────┴──────────┴─────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Data Cleaning and Preparation ---\n",
    "#\n",
    "# Professional Rationale:\n",
    "# We follow the methodology outlined in the JKP documentation and paper.\n",
    "# 1. Drop nulls: We require a valid characteristic (be_me), forward return (ret_exc_lead1m),\n",
    "#    and market equity (me) for our analysis.\n",
    "# 2. Exclude microcaps: The JKP paper (Section 2) and documentation (Section 2)\n",
    "#    state that portfolio breakpoints are based on \"non-micro stocks\" (those larger\n",
    "#    than the NYSE 20th percentile). We will apply this crucial screen.\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n--- Cleaning Data ---\")\n",
    "    initial_rows = df.shape[0]\n",
    "    print(f\"Initial row count: {initial_rows:,}\")\n",
    "\n",
    "    # Step 4.1: Drop rows with nulls in the columns essential for our analysis\n",
    "    df_filtered = df.drop_nulls(subset=[\"be_me\", \"me\", \"ret_exc_lead1m\"])\n",
    "    \n",
    "    rows_after_null_drop = df_filtered.shape[0]\n",
    "    print(f\"Row count after dropping nulls: {rows_after_null_drop:,}\")\n",
    "    \n",
    "    # Add the size group column from the raw data before dropping more rows\n",
    "    # The 'size_grp' column is pre-calculated by the JKP authors.\n",
    "    # We need to pull it from the original 'df' based on the 'id' and 'eom'\n",
    "    # We will need the original dataframe with the size_grp column. Let's re-run the ingestion to include it.\n",
    "    # For now, let's assume we have it and proceed.\n",
    "    # NOTE: This part highlights the iterative process of research. We will update the ingestion script later.\n",
    "    \n",
    "    # Step 4.2: Exclude micro-cap stocks as per the JKP methodology\n",
    "    # The documentation defines 'small' caps as stocks above the 20th percentile.\n",
    "    # Therefore, 'non-micro' includes 'small', 'large', and 'mega' caps.\n",
    "    # We need to know the values in the 'size_grp' column. Let's assume for now they are strings.\n",
    "    # This step is a placeholder until we add 'size_grp' to our data.\n",
    "    \n",
    "    # For the purpose of this exercise, let's add a placeholder for the size filter\n",
    "    # In a real scenario, we would go back and modify the data ingestion script first.\n",
    "    \n",
    "    # df_cleaned = df_filtered.filter(\n",
    "    #     pl.col(\"size_grp\").is_in([\"small\", \"large\", \"mega\"])\n",
    "    # )\n",
    "    \n",
    "    # Since we don't have the 'size_grp' column yet, for now our cleaned df is just the filtered one.\n",
    "    # This is an important point to note for the next step.\n",
    "    df_cleaned = df_filtered\n",
    "    \n",
    "    cleaned_rows = df_cleaned.shape[0]\n",
    "    print(f\"Final cleaned row count: {cleaned_rows:,}\")\n",
    "    print(f\"Total rows dropped: {initial_rows - cleaned_rows:,} ({ (initial_rows - cleaned_rows) / initial_rows:.2%})\")\n",
    "\n",
    "    # Verify that there are no more nulls in these columns\n",
    "    print(\"\\n--- Missing Value Counts After Cleaning ---\")\n",
    "    print(df_cleaned.select([\"be_me\", \"me\", \"ret_exc_lead1m\"]).null_count())\n",
    "\n",
    "    print(\"\\n--- Cleaned Data Head ---\")\n",
    "    print(df_cleaned.head().sort(\"eom\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd4d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qr_factor_replication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
